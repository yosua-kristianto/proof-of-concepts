{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DC-GAN (Deep Convolutional Generative Adversarial Network)\n",
    "\n",
    "I recently opened Tensorflow's tutorials on GAN (Generative Adversarial Network), and found this [link](https://www.tensorflow.org/tutorials/generative/dcgan). \n",
    "\n",
    "Since I still don't make any sense of what I did in TensorLayerX's API, I decided to rewrite the Tensorflow Keras version, in manner of I may be remember and getting to understand it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow;\n",
    "import matplotlib.pyplot as plt;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loading & Pre-Processing\n",
    "\n",
    "MNIST dataset is a database of handwritten digits for training. So people will type in 0 to 9 handwrittenly, taking the photo of it, and resizing it to 28 x 28 greyscale pixels.\n",
    "\n",
    "Tensorflow Keras API had an API to download MNIST dataset, and using it directly as train-test splits. If I'm not mistaken, TensorLayerX also had one, but I prefered this one since I once use it.\n",
    "\n",
    "By applying `.shape`, we can then see that by default, the training set had 60K worth of data while test set had 10K. From these 10K set, I split the test set into 50:50, making validation set from it.\n",
    "\n",
    "| Splits | Total data |\n",
    "|---|---|\n",
    "| Train | 60000 |\n",
    "| Test | 5000 |\n",
    "| Val | 5000 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist;\n",
    "import numpy;\n",
    "from tensorflow.data import Dataset;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (5000, 28, 28), (5000, 28, 28))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data();\n",
    "\n",
    "# Split the test and val by 50:50\n",
    "test_val_images_split = numpy.array_split(test_images, 2);\n",
    "test_val_labels_split = numpy.array_split(test_labels, 2);\n",
    "\n",
    "test_images = test_val_images_split[0];\n",
    "test_labels = test_val_labels_split[0];\n",
    "\n",
    "val_images = test_val_images_split[1];\n",
    "val_labels = test_val_labels_split[1];\n",
    "\n",
    "train_images.shape, test_images.shape, val_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Honestly I don't really understand why they didn't provide the way we use Test nor Val dataset so, I think I will be exploring it in other time. So here is dataset standard scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32');\n",
    "train_images = (train_images - 127.5) / 127.5;  # Normalize the images to [-1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Batching Configuration\n",
    "\n",
    "BUFFER_SIZE means total data within train_images\n",
    "BATCH_SIZE means the suitable Batch per epoch run. Change as you want.\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "This part will shuffle, and batch out the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 256)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "BUFFER_SIZE = train_images.shape[0];\n",
    "BATCH_SIZE = 256;\n",
    "\n",
    "BUFFER_SIZE, BATCH_SIZE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "\n",
    "GAN architecture consist of 2 Models. Generator, and Discriminator. \n",
    "\n",
    "Generator model inteded to be generate the *fake* hand written. By mean *fake*, means that the model will generate new hand written digit sample. \n",
    "Discriminator model intended for telling us whether the generated image are right or not. \n",
    "\n",
    "With these information aboves, we can assume that both generator and discriminator model must have at least these architectures as follow:\n",
    "\n",
    "1. Generator\n",
    "\n",
    "| Layer type | Specification | Purpose |\n",
    "|---|---|---|\n",
    "| Input Layer | Dense -> Shape (100, ) | This contains the random noise data for initializing the weight distribution for the model. |\n",
    "| Output Layer |  Conv2d -> Shape (28, 28, 1) | The drawing result from the model. |\n",
    "\n",
    "2. Discriminator\n",
    "\n",
    "| Layer type | Specification | Purpose |\n",
    "|---|---|---|\n",
    "| Input Layer | Conv2d -> Shape (28, 28, 1) | This is must be matched with Generator's output |\n",
    "| Output Layer | Linear -> Shape (1) | This is boolean. Truthy or falsy. Determine whether the generated image from Generator is fake or not. |\n",
    "\n",
    "Before we jumping into "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Conv2DTranspose, Reshape, BatchNormalization, Conv2D, Dense, Flatten;\n",
    "from tensorflow.keras.layers import LeakyReLU, ReLU;\n",
    "from tensorflow.keras.models import Sequential;\n",
    "from tensorflow.keras.initializers import TruncatedNormal;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G Model\n",
    "\n",
    "class MNIST_Model_G:\n",
    "    def __init__(self):\n",
    "        self.dense1 = Dense(units = 7 * 7 * 256, use_bias = False, name = \"dense1\", input_shape = (100, ));\n",
    "        self.bn1 = BatchNormalization(name = \"bn1\");\n",
    "        self.act1 = LeakyReLU(name = \"act1\");\n",
    "\n",
    "        self.reshape1 = Reshape((7, 7, 256));\n",
    "\n",
    "        self.convtrans1 = Conv2DTranspose(filters = 128, kernel_size = (5, 5), strides = (1, 1), padding = \"same\", use_bias = False, name = \"convtrans1\");\n",
    "        self.bn2 = BatchNormalization(name = \"bn2\");\n",
    "        self.act2 = LeakyReLU(name = \"act2\");\n",
    "\n",
    "        self.convtrans2 = Conv2DTranspose(filters = 64, kernel_size = (5, 5), strides = (2, 2), padding = \"same\", use_bias = False, name = \"convtrans2\");\n",
    "        self.bn3 = BatchNormalization(name = \"bn3\");\n",
    "        self.act3 = LeakyReLU(name = \"act3\");\n",
    "\n",
    "        self.output = Conv2DTranspose(filters = 1, kernel_size = (5, 5), strides = (2, 2), padding = \"same\", use_bias = False, name = \"output\");\n",
    "\n",
    "    def build_model(self):\n",
    "        model = Sequential(name = \"MNIST_Model_G.202405081658\");\n",
    "        \n",
    "        model.add(self.dense1);\n",
    "        model.add(self.bn1);\n",
    "        model.add(self.act1);\n",
    "\n",
    "        model.add(self.reshape1);\n",
    "\n",
    "        model.add(self.convtrans1);\n",
    "        model.add(self.bn2);\n",
    "        model.add(self.act2);\n",
    "\n",
    "        model.add(self.convtrans2);\n",
    "        model.add(self.bn3);\n",
    "        model.add(self.act3);\n",
    "\n",
    "        model.add(self.output);\n",
    "\n",
    "        return model;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yosua\\anaconda3\\envs\\tlx\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:86: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "mnist_G = MNIST_Model_G();\n",
    "generator = mnist_G.build_model();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1b7b8ffab40>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test noise\n",
    "noise = tensorflow.random.normal([1, 100]);\n",
    "\n",
    "generated_image = generator(noise, training = False);\n",
    "\n",
    "plt.imshow(generated_image[0, :, :, 0], cmap = \"gray\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D Model\n",
    "\n",
    "class MNIST_Model_D():\n",
    "    def __init__(self):\n",
    "        self.input = Input(shape = (28, 28, 1));\n",
    "\n",
    "        self.conv1 = Conv2D(filters = 64, kernel_size = (5, 5), strides = (2, 2), padding = \"same\");\n",
    "        self.act1 = LeakyReLU(name = \"act1\");\n",
    "\n",
    "        self.conv2 = Conv2D(filters = 128, kernel_size = (5, 5), strides = (2, 2), padding = \"same\");\n",
    "        self.act2 = LeakyReLU(name = \"act2\");\n",
    "\n",
    "        self.flat = Flatten(name = \"flat\");\n",
    "        self.output = Dense(units = 1);\n",
    "\n",
    "    def build_model(self):\n",
    "        model = Sequential(name = \"MNIST_Model_D.202405081658\");\n",
    "        model.add(self.input);\n",
    "\n",
    "        model.add(self.conv1);\n",
    "        model.add(self.act1);\n",
    "\n",
    "        model.add(self.conv2);\n",
    "        model.add(self.act2);\n",
    "\n",
    "        model.add(self.flat);\n",
    "        model.add(self.output);\n",
    "\n",
    "        return model;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.49996242]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "discriminator = MNIST_Model_D().build_model();\n",
    "\n",
    "decision = discriminator(generated_image);\n",
    "print (decision);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Discriminator Loss\n",
    "\n",
    "This method quantifies how well the discriminator is able to distinguish real images from fakes. It compares the discriminator's predictions on real images to an array of 1s, and the discriminator's predictions on fake (generated) images to an array of 0s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import BinaryCrossentropy;\n",
    "from tensorflow import ones_like;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_cross_entropy = BinaryCrossentropy(from_logits = True);\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = binary_cross_entropy(ones_like(real_output), real_output);\n",
    "    fake_loss = binary_cross_entropy(ones_like(fake_output), fake_output); \n",
    "\n",
    "    total_loss = real_loss + fake_loss;\n",
    "    return total_loss;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Generator Loss\n",
    "\n",
    "The generator's loss quantifies how well it was able to trick the discriminator. Intuitively, if the generator is performing well, the discriminator will classify the fake images as real (or 1). Here, compare the discriminators decisions on the generated images to an array of 1s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    return binary_cross_entropy(ones_like(fake_output), fake_output);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Optimization\n",
    "\n",
    "This part defining gradient descent for both Generator and Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_optim = Adam(learning_rate = 1e-3);\n",
    "D_optim = Adam(learning_rate = 1e-3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os;\n",
    "import tensorflow;\n",
    "\n",
    "checkpoint_dir = './training_checkpoints';\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tensorflow.train.Checkpoint(\n",
    "    generator_optimizer = G_optim,\n",
    "    discriminator_optimizer = D_optim,\n",
    "    generator = generator,\n",
    "    discriminator = discriminator\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial Training Session\n",
    "\n",
    "The special of GAN is the adversarial training process. While you train Discriminator like wise, the Generator model's backpropagation, is very dependent on the Discriminator's judgement. The logits that being generated by generator will be determined by its value whether the *wrong fake image* was spotted. This made GAN model cannot be trained using framework-wise like `fit` function in Tensorflow. This method is called as \"Train One Step\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Session Configuration\n",
    "EPOCHS = 300;\n",
    "noise_dimension = 100;\n",
    "sample_to_be_generated = 16;\n",
    "\n",
    "seed = tensorflow.random.normal([sample_to_be_generated, noise_dimension]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loop begins with generator receiving a random seed as input that was earlier called *noise*. That seed or *noise* is used to produce an image. \n",
    "\n",
    "The discriminator is then used to classify real images (drawn from the training set) and fakes images (produced by the generator). The loss is calculated for each of these models, and the gradients are used to update the generator and discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import GradientTape;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function needs to be compiled first before can be used.\n",
    "@tensorflow.function\n",
    "def train_one_step(image):\n",
    "    noise = tensorflow.random.normal([BATCH_SIZE, noise_dimension]);\n",
    "\n",
    "    with GradientTape() as generator_tape, GradientTape() as discriminator_tape:\n",
    "\n",
    "        # Generator model generate an image with loaded noise\n",
    "        generated_image = generator(noise, training = True);\n",
    "\n",
    "        # Getting the \"real\" logits\n",
    "        real_output = discriminator(image, training = True);\n",
    "        \n",
    "        # Getting the \"fake\" logits\n",
    "        fake_output = discriminator(generated_image, training = True);\n",
    "\n",
    "        # Output loss\n",
    "        G_loss = generator_loss(fake_output = fake_output);\n",
    "        D_loss = discriminator_loss(real_output = real_output, fake_output = fake_output);\n",
    "\n",
    "    # Define weight \n",
    "    G_weight = generator_tape.gradient(G_loss, generator.trainable_weights);\n",
    "    D_weight = discriminator_tape.gradient(D_loss, discriminator.trainable_weights);\n",
    "    \n",
    "    G_optim.apply_gradients(zip(G_weight, generator.trainable_weights));\n",
    "    D_optim.apply_gradients(zip(D_weight, discriminator.trainable_weights));\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Progress bar\n",
    "from tqdm import tqdm;\n",
    "from time import time;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the testing \n",
    "def generate_and_save_images(model, epoch, test_input):\n",
    "    # Notice `training` is set to False.\n",
    "    # This is so all layers run in inference mode (batchnorm).\n",
    "    predictions = model(test_input, training = False);\n",
    "\n",
    "    plt.figure(figsize=(4, 4));\n",
    "\n",
    "    for i in range(predictions.shape[0]):\n",
    "        plt.subplot(4, 4, i+1);\n",
    "        plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap = 'gray');\n",
    "        plt.axis('off');\n",
    "\n",
    "    plt.savefig('image_at_epoch_{:04d}.png'.format(epoch));\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train (dataset):\n",
    "    for i in range(EPOCHS):\n",
    "        start_time = time();\n",
    "\n",
    "        print(f\"Training Epoch {i + 1} / {EPOCHS}\", end = \" \");\n",
    "\n",
    "        for X_batch in tqdm(dataset):\n",
    "            train_one_step(X_batch);\n",
    "\n",
    "    # Produce Image for the GIF as you go\n",
    "    display.clear_output(wait = True);\n",
    "    generate_and_save_images(\n",
    "        generator,\n",
    "        EPOCHS,\n",
    "        seed\n",
    "    ); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Adversarial Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 1 / 300 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/235 [00:00<?, ?it/s]c:\\Users\\yosua\\anaconda3\\envs\\tlx\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\nn.py:695: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n",
      " 37%|███▋      | 86/235 [00:32<00:56,  2.64it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m)\u001b[49m;\n",
      "Cell \u001b[1;32mIn[49], line 8\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(dataset)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m / \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m);\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m X_batch \u001b[38;5;129;01min\u001b[39;00m tqdm(dataset):\n\u001b[1;32m----> 8\u001b[0m         \u001b[43mtrain_one_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m)\u001b[49m;\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Produce Image for the GIF as you go\u001b[39;00m\n\u001b[0;32m     11\u001b[0m display\u001b[38;5;241m.\u001b[39mclear_output(wait \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m);\n",
      "File \u001b[1;32mc:\\Users\\yosua\\anaconda3\\envs\\tlx\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\yosua\\anaconda3\\envs\\tlx\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\yosua\\anaconda3\\envs\\tlx\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:869\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    866\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    867\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    868\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 869\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    873\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    874\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    875\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\yosua\\anaconda3\\envs\\tlx\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yosua\\anaconda3\\envs\\tlx\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\yosua\\anaconda3\\envs\\tlx\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\Users\\yosua\\anaconda3\\envs\\tlx\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\yosua\\anaconda3\\envs\\tlx\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1500\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1498\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1500\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1501\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1503\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1504\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1505\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1506\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1508\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1509\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1510\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1514\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1515\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\yosua\\anaconda3\\envs\\tlx\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(train_dataset);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint.restore(tensorflow.train.latest_checkpoint(checkpoint_dir));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a single image using the epoch number\n",
    "def display_image(epoch_no):\n",
    "    return PIL.Image.open('image_at_epoch_{:04d}.png'.format(epoch_no))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_image(EPOCHS);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make training progress GIF using image.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anim_file = 'dcgan.gif'\n",
    "\n",
    "with imageio.get_writer(anim_file, mode='I') as writer:\n",
    "    filenames = glob.glob('image*.png');\n",
    "    filenames = sorted(filenames);\n",
    "    for filename in filenames:\n",
    "        image = imageio.imread(filename);\n",
    "        writer.append_data(image);\n",
    "    image = imageio.imread(filename);\n",
    "    writer.append_data(image);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_docs.vis.embed as embed\n",
    "embed.embed_file(anim_file);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tlx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
