{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DC-GAN (Deep Convolutional Generative Adversarial Network)\n",
    "\n",
    "I recently opened Tensorflow's tutorials on GAN (Generative Adversarial Network), and found this [link](https://www.tensorflow.org/tutorials/generative/dcgan). \n",
    "\n",
    "Basically the tutorial is making a generative handwritten digits image using Keras API. But in here, I'm gonna using TensorLayerX's API to build the GAN. Some parts may not following the Tensorflow's tutorial directly. \n",
    "\n",
    "Things to install:\n",
    "- tensorflow\n",
    "- tensorlayerx\n",
    "- torch (Just if you need it's backend. If not, you can skip torch)\n",
    "- matplotlib\n",
    "- numpy\n",
    "- pillow\n",
    "\n",
    "The goal here is to make a generative model that generates new handwritten digit images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow;\n",
    "from tensorflow.keras.datasets import mnist;\n",
    "import numpy;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch;\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loading & Pre-Processing\n",
    "\n",
    "MNIST dataset is a database of handwritten digits for training. So people will type in 0 to 9 handwrittenly, taking the photo of it, and resizing it to 28 x 28 greyscale pixels.\n",
    "\n",
    "Tensorflow Keras API had an API to download MNIST dataset, and using it directly as train-test splits. If I'm not mistaken, TensorLayerX also had one, but I prefered this one since I once use it.\n",
    "\n",
    "By applying `.shape`, we can then see that by default, the training set had 60K worth of data while test set had 10K. From these 10K set, I split the test set into 50:50, making validation set from it.\n",
    "\n",
    "| Splits | Total data |\n",
    "|---|---|\n",
    "| Train | 60000 |\n",
    "| Test | 5000 |\n",
    "| Val | 5000 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (5000, 28, 28), (5000, 28, 28))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data();\n",
    "\n",
    "# Split the test and val by 50:50\n",
    "test_val_images_split = numpy.array_split(test_images, 2);\n",
    "test_val_labels_split = numpy.array_split(test_labels, 2);\n",
    "\n",
    "test_images = test_val_images_split[0];\n",
    "test_labels = test_val_labels_split[0];\n",
    "\n",
    "val_images = test_val_images_split[1];\n",
    "val_labels = test_val_labels_split[1];\n",
    "\n",
    "train_images.shape, test_images.shape, val_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "With the done of the data preprocessing, I will do Data Preparation using TensorLayerX's API. This part is crucial since we can't just set the data into the model without reshaping it into proper tensor format.\n",
    "\n",
    "As mentioned above, we know that the images consists of data dimension below:\n",
    "\n",
    "| Attribute | Size |\n",
    "|---|---|\n",
    "| Width | 28px |\n",
    "| Height | 28px |\n",
    "| Color Channels | Gray Scale |\n",
    "\n",
    "With this information above, we can conclude that the tensor dimension should be (28 -> Width, 28 -> Height, 1 -> Color Channel). Which is, not same as the loaded version that was (28, 28).\n",
    "\n",
    "The data preparation is done using Data Loading pattern. This pattern will encapsulating the process of loading and preprocessing data. Since the rest of data pre-processing phase below will manipulating the nature of the data, like converting it to some data format, it is best to encapsulating it into a class. \n",
    "\n",
    "So what we will do in this part are:\n",
    "\n",
    "1. Set Tensorlayerx's backend as Tensorflow / Torch\n",
    "2. Create Data Loader class\n",
    "3. Converts every value within features with shape of (28, 28, 1) and as float32 tensor since it is easier to model to read.\n",
    "4. Standardize the data within features by dividing it to 255.\n",
    "5. Register all train-test-val data with Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "c:\\Users\\yosua\\anaconda3\\envs\\tlx-obor\\Lib\\site-packages\\tensorlayerx\\__init__.py:45: UserWarning: The version of the backend you have installed does not match the specified backend version and may not work, please install version tensorflow 2.4.0.\n",
      "  warnings.warn(\"The version of the backend you have installed does not match the specified backend version \"\n"
     ]
    }
   ],
   "source": [
    "import tensorlayerx;\n",
    "from tensorlayerx import expand_dims, convert_to_tensor, float32, squeeze;\n",
    "from tensorlayerx.dataflow import IterableDataset;\n",
    "\n",
    "import os;\n",
    "\n",
    "# Set Tensorlayerx's backend as Tensorflow / Torch\n",
    "# os.environ[\"TL_BACKEND\"] = \"tensorflow\"; # Uncomment this line to use Tensorflow's Backend\n",
    "os.environ[\"TL_BACKEND\"] = \"torch\"; # Uncomment this line to use Torch's backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Data Loader class\n",
    "class DatasetLoader(IterableDataset):\n",
    "    def __init__(self, feature, label):\n",
    "\n",
    "        # Converts every value within features with shape of (28, 28, 1) and as float32 tensor since it is easier to model to read.\n",
    "        # Standardize the data within features by dividing it to 255.\n",
    "        self.data = feature.reshape((len(feature), 28, 28, 1)).astype('float32') / 255;\n",
    "        self.label = label.astype('int32');\n",
    "    \n",
    "        print(f\"Data Shape: {self.data.shape}\");\n",
    "        print(f\"Label Shape: {self.label.shape}\");\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.data[index];\n",
    "        label = self.label[index];\n",
    "\n",
    "        return data, label;\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data);\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i in range(len(self.data)):\n",
    "            yield self.data[i], self.label[i];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Shape: (60000, 28, 28, 1)\n",
      "Label Shape: (60000,)\n",
      "Data Shape: (5000, 28, 28, 1)\n",
      "Label Shape: (60000,)\n",
      "Data Shape: (5000, 28, 28, 1)\n",
      "Label Shape: (60000,)\n"
     ]
    }
   ],
   "source": [
    "# Register all train-test-val data with Data Loader\n",
    "\n",
    "train_set = DatasetLoader(train_images, train_labels);\n",
    "test_set = DatasetLoader(test_images, train_labels);\n",
    "val_set = DatasetLoader(val_images, train_labels);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((28, 28, 1), 5)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, label = val_set.__getitem__(0);\n",
    "\n",
    "data.shape, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "\n",
    "GAN architecture consist of 2 Models. Generator, and Discriminator. \n",
    "\n",
    "Discriminator model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorlayerx.nn import Module, Conv2d, Linear, BatchNorm, MaxPool2d, Flatten;\n",
    "from tensorlayerx import LeakyReLU;\n",
    "from tensorlayerx.metrics import Accuracy;\n",
    "from tensorlayerx.losses import binary_cross_entropy;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator Model\n",
    "\n",
    "class MNIST_Model_G(Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MNIST_Model_G, self).__init__();\n",
    "\n",
    "        # I don't know why this can't be from tlx.initializers import TruncatedNormal\n",
    "        w_init = tlx.initializers.TruncatedNormal(stddev = 0.02);\n",
    "        b_init = tlx.initializers.TruncatedNormal(mean = 1.0, stddev = 0.02);\n",
    "\n",
    "        self.input = Input(shape = (64, 28, 28, 1));\n",
    "        self.conv1 = Conv2d(out_channels = 64, kernel_size = (3, 3), act = LeakyReLU, padding = \"SAME\", W_init = w_init, b_init = b_init, data_format = \"channel_first\", name = \"conv1\");\n",
    "        self.conv2 = Conv2d(out_channels = 64, kernel_size = (3, 3), act = LeakyReLU, padding = \"SAME\", W_init = w_init, b_init = b_init, data_format = \"channel_first\", name = \"conv2\");\n",
    "        \n",
    "        self.output = Conv2d(out_channels = 1, kernel_size = (3, 3), act = LeakyReLU, padding = \"SAME\", W_init = w_init, b_init = b_init, data_format = \"channel_first\", name = \"output\");\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x);\n",
    "        x = self.conv2(x);\n",
    "        x = self.output(x);\n",
    "\n",
    "        return x;\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = self.input(x);\n",
    "        x = self.conv1(x);\n",
    "        x = self.conv2(x);\n",
    "        out = self.output(x);\n",
    "\n",
    "        return out;\n",
    "\n",
    "class G_With_Loss(Module):\n",
    "    def __init__(self, network: Module, loss_fn):\n",
    "        super(G_With_Loss, self).__init__();\n",
    "\n",
    "        self.network = network;\n",
    "        self.loss_function = loss_fn;\n",
    "\n",
    "    def forward(self, data, ground_truth):\n",
    "        predictions = self.network(data);\n",
    "        loss = self.loss_function(predictions, ground_truth);\n",
    "\n",
    "        return loss;\n",
    "\n",
    "G_network = MNIST_Model_G();\n",
    "G_netW_Loss = G_With_loss(network = G_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_Model_D(Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MNIST_Model_D, self).__init__();\n",
    "\n",
    "        # I don't know why this can't be from tlx.initializers import TruncatedNormal\n",
    "        w_init = tlx.initializers.TruncatedNormal(stddev = 0.02);\n",
    "        b_init = tlx.initializers.TruncatedNormal(mean = 1.0, stddev = 0.02);\n",
    "\n",
    "        self.input = Input(shape = (28, 28, 1));\n",
    "        \n",
    "        self.conv1 = Conv2d(out_channels = 64, kernel_size = (3, 3), act = LeakyReLU, padding = \"SAME\", W_init = w_init, b_init = b_init, data_format = \"channel_first\", name = \"conv1\");\n",
    "        self.pool1 = MaxPool2d(kernel_size = (2, 2), name = \"pool1\");\n",
    "\n",
    "        self.conv2 = Conv2d(out_channels = 32, kernel_size = (3, 3), act = LeakyReLU, padding = \"SAME\", W_init = w_init, b_init = b_init, data_format = \"channel_first\", name = \"conv2\");\n",
    "        self.pool2 = MaxPool2d(kernel_size = (2, 2), name = \"pool2\");\n",
    "\n",
    "        self.flat = Flatten(name = \"flat\");\n",
    "        self.output = Linear(\n",
    "            out_features = 1,\n",
    "            W_init = tlx.initalizers.TruncatedNormal(stddev = 5-e2),\n",
    "            b_init = tlx.initalizers.TruncatedNormal(mean = 1, stddev = 2e-2),\n",
    "            act = LeakyReLU,\n",
    "            name = \"output\"\n",
    "        );\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x);\n",
    "        x = self.pool1(x);\n",
    "        \n",
    "        x = self.conv2(x);\n",
    "        x = self.pool2(x);\n",
    "\n",
    "        x = self.flat(x);\n",
    "        x = self.output(x);\n",
    "\n",
    "        return x;\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = self.input(x);\n",
    "\n",
    "        x = self.conv1(x);\n",
    "        x = self.pool1(x);\n",
    "        \n",
    "        x = self.conv2(x);\n",
    "        x = self.pool2(x);\n",
    "\n",
    "        x = self.flat(x);\n",
    "        out = self.output(x);\n",
    "\n",
    "        return out;\n",
    "\n",
    "class D_With_Loss(Module):\n",
    "    def __init__(self, network: Module, loss_fn):\n",
    "        super(D_With_Loss, self).__init__();\n",
    "\n",
    "        self.network = network;\n",
    "        self.loss_function = loss_fn;\n",
    "\n",
    "    def forward(self, data, ground_truth):\n",
    "        predictions = self.network(data);\n",
    "        loss = self.loss_function(predictions, ground_truth);\n",
    "\n",
    "        return loss;\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tlx-obor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
